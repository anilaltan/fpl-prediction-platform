{
  "master": {
    "tasks": [
      {
        "id": "13",
        "title": "Setup Test Infrastructure and Configuration",
        "description": "Initialize the testing environment with Pytest, async fixtures, and a dedicated test database configuration to support the test pyramid strategy.",
        "details": "Create `tests/conftest.py` with session-scoped and function-scoped fixtures. Configure a separate test database URL in `config.py` to ensure tests do not pollute the development/production database. Install `pytest`, `pytest-asyncio`, and `hypothesis` if not present. Create the folder structure `tests/unit`, `tests/integration`, `tests/performance`. Ensure `backend` is importable by tests (e.g., via `PYTHONPATH` or `setup.py` editable install).",
        "testStrategy": "Run `pytest` with a simple dummy test to verify configuration and database connection/teardown logic.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install Testing Dependencies",
            "description": "Add necessary testing libraries to requirements and install them in the environment.",
            "dependencies": [],
            "details": "Update the project's dependency management file (e.g., requirements.txt or pyproject.toml) to include 'pytest', 'pytest-asyncio', 'httpx' (for async API testing), and 'hypothesis'. Run the installation command to ensure packages are available in the development environment.",
            "status": "done",
            "testStrategy": "Verify installation by running 'pytest --version' in the command line.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:09:50.743Z"
          },
          {
            "id": 2,
            "title": "Configure Test Environment Variables",
            "description": "Update the configuration module to support a separate test database URL.",
            "dependencies": [
              1
            ],
            "details": "Modify `backend/app/core/config.py` (or equivalent config file found in analysis) to load a `TEST_DATABASE_URL` from environment variables, defaulting to a local SQLite or separate PostgreSQL instance if not provided. Ensure the `Settings` class can distinguish between 'testing', 'development', and 'production' environments.",
            "status": "done",
            "testStrategy": "Import the settings in a python shell and verify that setting `MODE=TEST` changes the database URL to the test version.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:09:50.752Z"
          },
          {
            "id": 3,
            "title": "Create Test Directory Structure",
            "description": "Initialize the standard testing folder hierarchy to support the test pyramid.",
            "dependencies": [
              1
            ],
            "details": "Create the root `tests` directory if it doesn't exist. Inside it, create subdirectories: `tests/unit`, `tests/integration`, `tests/performance`, and `tests/e2e`. Add `__init__.py` files where necessary to ensure test discovery works correctly.",
            "status": "done",
            "testStrategy": "Run `ls -R tests` (or equivalent) to verify the directory structure is created.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:09:50.762Z"
          },
          {
            "id": 4,
            "title": "Implement Pytest Configuration and Fixtures",
            "description": "Set up `conftest.py` with essential async fixtures for database and app instance management.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create `tests/conftest.py`. Define a `session`-scoped fixture to create/drop the test database schema. Define a `function`-scoped fixture to provide an async database session that rolls back changes after each test. Define an `async_client` fixture using `httpx.AsyncClient` for testing FastAPI endpoints.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:09:50.771Z"
          },
          {
            "id": 5,
            "title": "Verify Setup with Smoke Test",
            "description": "Create a simple dummy test to validate the testing infrastructure and database connection.",
            "dependencies": [
              4
            ],
            "details": "Create `tests/unit/test_smoke.py`. Add a simple async test function that asserts 1 + 1 equals 2, and another test that resolves a simple dependency or queries the DB session to ensure fixtures are wiring up correctly without errors.",
            "status": "done",
            "testStrategy": "Run `pytest` and ensure the smoke tests pass and the test database is correctly initialized and torn down.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:09:50.780Z"
          }
        ],
        "updatedAt": "2026-01-25T23:09:50.780Z"
      },
      {
        "id": "14",
        "title": "Implement ETL Transaction Safety and Locking",
        "description": "Enhance the ETL service to prevent race conditions and ensure data integrity during concurrent updates.",
        "details": "Modify `backend/app/services/etl/service.py`. Implement explicit database transaction blocks (e.g., `async with transaction.atomic()` or SQLAlchemy session usage). Add `SELECT FOR UPDATE` logic when fetching records that will be modified. Introduce an `asyncio.Lock` for in-memory cache updates if applicable. Ensure that simultaneous calls to update the same player data do not result in duplicate entries or partial writes.",
        "testStrategy": "Create an integration test spawning concurrent ETL tasks targeting the same resource and verify that the final state is consistent and no DB deadlocks occur.",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor ETL Service for Atomic Transactions",
            "description": "Establish explicit transaction boundaries within the ETL service methods to ensure atomicity.",
            "dependencies": [],
            "details": "Modify `backend/app/services/etl/service.py`. Refactor the main entry points (e.g., `update_all_data`, `refresh_players`) to wrap their logic in a database transaction block (e.g., `async with db.transaction():` or `session.begin()`). Ensure that if an error occurs during the ETL process, all database changes are rolled back to maintain consistency.",
            "status": "done",
            "testStrategy": "Unit test verifying that raising an exception inside the transaction block prevents data persistence.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:40:11.539Z"
          },
          {
            "id": 2,
            "title": "Implement Row-Level Locking for Updates",
            "description": "Prevent concurrent modification of the same records by locking rows during fetch.",
            "dependencies": [
              1
            ],
            "details": "In `backend/app/services/etl/service.py`, identify queries that fetch entities (Players, Teams) for the purpose of updating them. Append `.with_for_update()` (if using SQLAlchemy) or inject `FOR UPDATE` clauses to these queries to lock the selected rows until the transaction commits.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:40:11.549Z"
          },
          {
            "id": 3,
            "title": "Implement Safe Upsert Logic",
            "description": "Replace check-then-insert logic with atomic upsert operations to handle unique constraints.",
            "dependencies": [
              1
            ],
            "details": "Refactor the insertion logic in `backend/app/services/etl/service.py`. Instead of checking if a record exists and then inserting, use database-native upsert constructs (e.g., PostgreSQL `INSERT ... ON CONFLICT DO UPDATE`) to prevent `IntegrityError` or duplicates when multiple workers process the same data simultaneously.",
            "status": "done",
            "testStrategy": "Unit test ensuring that inserting the same unique key twice updates the existing record rather than failing.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:40:11.558Z"
          },
          {
            "id": 4,
            "title": "Add In-Memory Locking for Cache Updates",
            "description": "Synchronize access to shared in-memory structures within the ETL service instance.",
            "dependencies": [],
            "details": "Initialize `self._lock = asyncio.Lock()` in the `ETLService` constructor in `backend/app/services/etl/service.py`. Wrap any modifications to internal state caches (e.g., mapping dictionaries for teams or positions) with `async with self._lock:` to ensure thread safety during concurrent execution.",
            "status": "done",
            "testStrategy": null,
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:40:11.566Z"
          },
          {
            "id": 5,
            "title": "Create Concurrency Integration Tests",
            "description": "Verify that the ETL service handles concurrent execution without data corruption or deadlocks.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create a new test file `tests/integration/test_etl_concurrency.py`. Implement a test using `asyncio.gather` to launch multiple concurrent calls to the ETL update method for the same player/resource. Assert that the final database state is correct, contains no duplicates, and that no unhandled database lock exceptions occurred.",
            "status": "done",
            "testStrategy": "Execute the concurrency test suite against a test database instance.",
            "parentId": "undefined",
            "updatedAt": "2026-01-25T23:40:11.574Z"
          }
        ],
        "updatedAt": "2026-01-25T23:40:11.574Z"
      },
      {
        "id": "15",
        "title": "Refactor Feature Engineering for Unit Testability",
        "description": "Decouple feature engineering logic from the execution engine to allow for pure function unit testing.",
        "details": "Identify feature calculation logic in `backend/app/services/ml/`. Extract these into pure functions (e.g., `calculate_rolling_average(stats_list) -> float`) that do not depend on DB sessions or external services. Ensure these functions handle edge cases (empty lists, None values).",
        "testStrategy": "Write unit tests in `tests/unit/test_features.py` covering various input scenarios including boundary values, ensuring 100% coverage for these calculations.",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Pure Calculation Module",
            "description": "Create a new dedicated module for pure mathematical functions to separate logic from data access.",
            "dependencies": [],
            "details": "Create `backend/app/services/ml/calculations.py`. Define the structure for pure functions that accept standard Python types (lists, dicts, floats) or Pydantic models, strictly avoiding SQLAlchemy ORM objects or database sessions as arguments.",
            "status": "done",
            "testStrategy": "Verify the module is importable and contains no imports from `sqlalchemy` or the `db` module.",
            "parentId": "undefined",
            "updatedAt": "2026-01-26T00:27:52.850Z"
          },
          {
            "id": 2,
            "title": "Refactor Historical and Rolling Metric Logic",
            "description": "Extract complex time-series logic like rolling averages into the new pure calculation module.",
            "dependencies": [
              1
            ],
            "details": "Identify logic in `backend/app/services/ml/` responsible for calculating trends (e.g., average points last 5 gameweeks). Move this into `calculations.py` as functions like `calculate_rolling_avg(values: list[float], window: int) -> float`. Implement checks for empty lists.",
            "status": "done",
            "testStrategy": "Create unit tests passing list arrays representing match history and verifying correct average calculations.",
            "parentId": "undefined",
            "updatedAt": "2026-01-26T00:27:52.858Z"
          },
          {
            "id": 3,
            "title": "Refactor Derived Static Metric Logic",
            "description": "Extract single-point derived metrics calculation logic into the pure module.",
            "dependencies": [
              1
            ],
            "details": "Move logic for ratios and standardizations (e.g., `goals_per_90`, `creativity_rank_score`) into `calculations.py`. Ensure functions handle division by zero (e.g., if minutes played is 0) by returning 0.0 or valid defaults.",
            "status": "done",
            "testStrategy": "Create unit tests passing specific stat dictionaries to verify correct ratio computations and zero-division handling.",
            "parentId": "undefined",
            "updatedAt": "2026-01-26T00:27:52.866Z"
          },
          {
            "id": 4,
            "title": "Update Feature Service Orchestration",
            "description": "Modify the existing feature service to fetch data first, then call the new pure functions.",
            "dependencies": [
              2,
              3
            ],
            "details": "Refactor `backend/app/services/ml/feature_engineering.py` (or equivalent service file). Replace inline math with calls to `calculations.py`. The service should now strictly focus on querying the database and preparing the raw data structures required by the calculation functions.",
            "status": "done",
            "testStrategy": "Run integration tests to ensure the full pipeline still produces the expected feature set values when connected to the DB.",
            "parentId": "undefined",
            "updatedAt": "2026-01-26T00:27:52.875Z"
          },
          {
            "id": 5,
            "title": "Implement Unit Tests for Calculation Module",
            "description": "Create a comprehensive unit test suite for the extracted pure functions covering edge cases.",
            "dependencies": [
              2,
              3
            ],
            "details": "Create `tests/unit/test_features.py`. Import functions from `backend/app/services/ml/calculations.py`. Write test cases for normal inputs, empty lists, `None` values, and zeros. Ensure 100% code coverage for the calculation logic.",
            "status": "done",
            "testStrategy": "Run `pytest tests/unit/test_features.py` and verify all tests pass without requiring a database connection.",
            "parentId": "undefined",
            "updatedAt": "2026-01-26T00:27:52.881Z"
          }
        ],
        "updatedAt": "2026-01-26T00:27:52.881Z"
      },
      {
        "id": "16",
        "title": "Implement ML Model Functional Validation (Smoke Test)",
        "description": "Add a verification layer to ensure loaded ML models produce non-zero, valid predictions before serving traffic.",
        "details": "Update `backend/app/services/ml/engine.py`. Create a `validate_models_loaded()` function. Upon application startup or model load, run a dummy prediction with mock input data. If the result is `0.0`, `NaN`, or an error occurs, raise a critical `ModelError` and prevent the service from reporting 'healthy'.",
        "testStrategy": "Unit test the loader with a mock model that returns 0.0 to ensure the exception is raised. Integration test the startup sequence to verify it fails fast on bad models.",
        "priority": "high",
        "dependencies": [
          "15"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define ModelError Exception Class",
            "description": "Create a specific exception class to handle model validation failures.",
            "dependencies": [],
            "details": "Create or update `backend/app/core/exceptions.py` (or define within `backend/app/services/ml/errors.py` if it exists) to include a `ModelError` class. This exception should be used specifically when ML models fail functional validation tests (e.g., producing NaN or zero output on test vectors).",
            "status": "pending",
            "testStrategy": "Verify the exception can be imported and raised.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Mock Prediction Data Generators",
            "description": "Implement utility functions to generate schema-compliant mock input data for model testing.",
            "dependencies": [
              1
            ],
            "details": "In `backend/app/services/ml/engine.py`, implement a helper function `_get_smoke_test_input()` that returns a pandas DataFrame or dictionary matching the exact feature schema required by the ML model. Ensure the data types (float, int, category) match what the model expects to prevent TypeErrors during the smoke test.",
            "status": "pending",
            "testStrategy": "Unit test the helper function to ensure it returns a valid data structure with the correct shape and columns.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement validate_models_loaded Function",
            "description": "Add the core validation logic to the ML engine to test models against mock data.",
            "dependencies": [
              2
            ],
            "details": "Update `backend/app/services/ml/engine.py` to include `validate_models_loaded(model_artifact)`. This function should invoke `model.predict()` using the mock input from the previous subtask. It must check if the output is `NaN`, `Inf`, `0.0` (if 0.0 is invalid for the context), or raises an exception. If invalid, raise `ModelError`.",
            "status": "pending",
            "testStrategy": "Unit test with a mocked model object: one that returns valid data (pass), one that returns NaN (fail), and one that returns 0.0 (fail).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Validation into Model Loading Lifecycle",
            "description": "Ensure the validation runs automatically immediately after a model is loaded into memory.",
            "dependencies": [
              3
            ],
            "details": "Modify the `load_model` (or equivalent initialization) method in `backend/app/services/ml/engine.py`. Immediately after `joblib.load()` or `pickle.load()`, call `self.validate_models_loaded(loaded_model)`. If validation fails, log a critical error and abort the assignment of the model to the active service instance.",
            "status": "pending",
            "testStrategy": "Integration test: Attempt to initialize the ML service with a 'corrupt' model file (or mocked loader) and verify the service fails to start up or reports an error state.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Update Health Check Endpoint to Reflect Model Status",
            "description": "Connect the ML model status to the application's health reporting mechanism.",
            "dependencies": [
              4
            ],
            "details": "Update the health check endpoint (likely in `backend/app/api/routes/health.py` or similar) and the ML service state. Ensure that if the model failed validation during startup, the readiness probe returns 503 Service Unavailable instead of 200 OK. Add a flag `self.is_ready` in the ML service that is only set to True after successful validation.",
            "status": "pending",
            "testStrategy": "Request the health endpoint before model load (should be not ready), after successful load (ready), and after failed load (not ready).",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "17",
        "title": "Decouple Solver Constraints for Isolated Testing",
        "description": "Extract constraint logic from the main solver class to allow independent verification of optimization rules.",
        "details": "Refactor the solver (likely `backend/app/services/team_optimizer.py` or similar). Move constraints like 'budget limit', 'max players per team', and 'position limits' into separate validator functions or a `ConstraintFactory`. This allows testing constraints without running the full heavy ILP solver.",
        "testStrategy": "Unit tests verifying that specific invalid team compositions correctly trigger constraint violations.",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Analyze and Map Existing Solver Constraints",
            "description": "Review the optimization service to identify all hard-coded constraints currently implemented within the main solve method.",
            "dependencies": [],
            "details": "Examine `backend/app/services/team_optimizer.py` (or the equivalent file found via analysis). List all PuLP constraints such as budget limits, squad size (15 players), position counts (2 GKP, 5 DEF, etc.), and team-specific limits (max 3 players per club). Document the input variables required for each.",
            "status": "pending",
            "testStrategy": null,
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create ConstraintFactory Interface and Base Class",
            "description": "Establish a new module structure to house the decoupled constraint logic.",
            "dependencies": [
              1
            ],
            "details": "Create a new file `backend/app/services/optimization/constraints.py`. Define an abstract base class or interface for constraints that accepts the PuLP problem instance and the decision variables dictionary. This standardizes how constraints are applied to the linear programming problem.",
            "status": "pending",
            "testStrategy": "Unit test to verify the interface can be instantiated or mocked.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Individual Constraint Classes",
            "description": "Move the logic for specific constraints into separate classes implementing the new interface.",
            "dependencies": [
              2
            ],
            "details": "Refactor logic for `BudgetConstraint`, `SquadSizeConstraint`, `PositionLimitConstraint`, and `TeamLimitConstraint` into their own classes in the new module. Each class should have an `apply(problem, lp_variables, data)` method. Ensure pure logic is separated so it can be tested without the solver where possible.",
            "status": "pending",
            "testStrategy": "Unit tests for each constraint class verifying they add the correct number of rows to a mock PuLP problem.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Refactor Main Solver to Use ConstraintFactory",
            "description": "Update the main optimizer service to use the new constraint classes instead of inline logic.",
            "dependencies": [
              3
            ],
            "details": "Modify `TeamOptimizer` to instantiate and iterate through the constraint classes. Remove the old inline constraint code. Inject the configuration (e.g., max budget, max players) into these classes rather than hardcoding them in the solver method.",
            "status": "pending",
            "testStrategy": "Integration test running the full solver to ensure it still produces valid teams with the refactored code.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Validation Tests for Isolated Constraints",
            "description": "Create a suite of unit tests that specifically verify constraint logic independently of the solver engine.",
            "dependencies": [
              4
            ],
            "details": "Create `tests/unit/test_constraints.py`. detailed tests where we pass specific sets of dummy players and variables to the constraint classes and assert that the mathematical expressions generated match expected inequalities (e.g., sum of costs <= 1000).",
            "status": "pending",
            "testStrategy": "Pytest execution of the new unit test file.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "18",
        "title": "Harden Solver with Timeouts and Error Handling",
        "description": "Configure the PuLP solver to handle large problem instances gracefully by enforcing time limits.",
        "details": "In the Team Optimization service, configure the solver (e.g., `prob.solve(PULP_CBC_CMD(timeLimit=30))`). Wrap the solve execution in a try-except block to catch `PulpSolverError` or timeout exceptions. Return a structured error response instead of hanging indefinitely if the solution isn't found within 30 seconds.",
        "testStrategy": "Performance test using a mock large dataset or artificially restricted time limit (e.g., 0.01s) to ensure the timeout logic triggers and returns the correct error object.",
        "priority": "high",
        "dependencies": [
          "17"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Solver-Specific Custom Exceptions",
            "description": "Create a dedicated exception hierarchy for the optimization solver to handle specific failure scenarios distinct from general application errors.",
            "dependencies": [],
            "details": "Create or update `backend/app/core/exceptions.py` (or a new `solver_exceptions.py`) to include `SolverTimeoutError`, `SolverInfeasibleError`, and `SolverExecutionError`. Ensure these inherit from a base `AppException` so they can be caught by the global exception handler later.",
            "status": "pending",
            "testStrategy": "Verify that new exceptions can be instantiated and caught within a basic try-except block.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Configure PuLP Solver with Time Limits",
            "description": "Modify the solver initialization to accept and enforce a strict execution time limit.",
            "dependencies": [
              1
            ],
            "details": "In the team optimization service (e.g., `backend/app/services/team_optimizer.py`), update the `prob.solve()` call. Instead of the default solver, instantiate `PULP_CBC_CMD` (or the configured solver) with `timeLimit=30` (seconds) and `msg=0` (to reduce log noise). Make the timeout value configurable via environment variables or settings.",
            "status": "pending",
            "testStrategy": "Run the solver with a very short timeout (e.g., 0.01s) on a standard problem to confirm it respects the configuration parameters.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Wrapper for Solver Execution",
            "description": "Wrap the core solving logic in a robust error-handling block to manage runtime failures.",
            "dependencies": [
              2
            ],
            "details": "Refactor the solve method to wrap `prob.solve()` in a `try...except` block. Catch `PulpSolverError` and generic `Exception`, logging the specific error details, and raising `SolverExecutionError`. Ensure the system does not crash on internal solver failures.",
            "status": "pending",
            "testStrategy": "Mock the `prob.solve` method to raise a `PulpSolverError` and verify that `SolverExecutionError` is raised with appropriate logs.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Validate Solver Status and Handle Non-Optimal Results",
            "description": "Explicitly check the solution status code returned by PuLP and handle non-optimal states gracefully.",
            "dependencies": [
              2,
              3
            ],
            "details": "After `prob.solve()`, check `LpStatus[prob.status]`. If the status is not 'Optimal': \n1. If 'Not Solved' or time limit reached, raise `SolverTimeoutError`.\n2. If 'Infeasible', raise `SolverInfeasibleError`.\nEnsure specific error messages help debug why a solution wasn't found (e.g., 'Constraints too strict').",
            "status": "pending",
            "testStrategy": "Create a test case with impossible constraints (e.g., cost < 0) and verify that `SolverInfeasibleError` is raised.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Error Responses in API Layer",
            "description": "Ensure the API endpoint calling the solver translates solver exceptions into appropriate HTTP responses.",
            "dependencies": [
              4
            ],
            "details": "In the API router (e.g., `backend/app/api/endpoints/optimization.py`), ensure the endpoint catches `SolverTimeoutError` (return HTTP 408), `SolverInfeasibleError` (return HTTP 422), and `SolverExecutionError` (return HTTP 500). Return a structured JSON error response.",
            "status": "pending",
            "testStrategy": "Integration test: Call the API endpoint with parameters triggering a timeout and infeasibility to verify the HTTP status codes and error messages.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "19",
        "title": "Implement Property-Based Testing for Solver",
        "description": "Use Hypothesis to strictly verify that generated solutions always satisfy critical constraints.",
        "details": "Create `tests/unit/test_solver_properties.py`. Define strategies using `hypothesis` to generate random player sets, budgets, and constraints. Assert that for every valid solution returned by the solver, the total cost <= budget, squad size == 15, etc. This mitigates the risk of non-deterministic or invalid solutions.",
        "testStrategy": "Run the property-based test suite. It should generate hundreds of scenarios automatically to find edge cases where constraints might be violated.",
        "priority": "medium",
        "dependencies": [
          "17",
          "18"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup Hypothesis and Test Scaffold",
            "description": "Install the Hypothesis library and initialize the test file for property-based testing.",
            "dependencies": [],
            "details": "Add 'hypothesis' to the project's test dependencies (e.g., requirements.txt or poetry). Create a new file 'tests/unit/test_solver_properties.py' and import necessary modules including 'hypothesis', 'hypothesis.strategies', and the solver service.",
            "status": "pending",
            "testStrategy": "Verify that 'pytest tests/unit/test_solver_properties.py' runs without import errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Define Player Data Generation Strategies",
            "description": "Create Hypothesis strategies to generate synthetic lists of FPL players with valid attributes.",
            "dependencies": [
              1
            ],
            "details": "In 'test_solver_properties.py', define a composite strategy using 'st.builds' or dictionaries to generate 'Player' objects. Ensure valid ranges for 'now_cost' (e.g., 35-140), 'element_type' (1-4 for GKP, DEF, MID, FWD), and 'team' (1-20). Ensure the generated list has enough players to form a valid squad.",
            "status": "pending",
            "testStrategy": "Write a temporary test that generates a sample list of players and asserts that attribute types and ranges are correct.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Define Budget and Constraint Strategies",
            "description": "Create strategies for variable input parameters such as total budget and team limits.",
            "dependencies": [
              1
            ],
            "details": "Define strategies for the budget parameter (e.g., 'st.integers(min_value=800, max_value=1050)') and max players per team constraints. This ensures the solver is tested against tight, loose, and standard budgetary constraints.",
            "status": "pending",
            "testStrategy": "Verify that the strategies produce values within the expected boundaries when sampled.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Financial and Size Property Tests",
            "description": "Write property tests to verify that solutions never exceed the budget and always return the correct squad size.",
            "dependencies": [
              2,
              3
            ],
            "details": "Use '@given(players=..., budget=...)' to drive the solver. Assert that if a valid solution is returned: 1) The sum of selected players' costs is less than or equal to the provided budget. 2) The total number of selected players is exactly 15 (or the configured squad size).",
            "status": "pending",
            "testStrategy": "Run the test suite. It should pass for valid logic and fail if the solver allows overspending.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Compositional Rule Property Tests",
            "description": "Write property tests to verify positional requirements and team limits.",
            "dependencies": [
              2,
              3
            ],
            "details": "Extend the assertions to check detailed rules: 1) Position counts must match standard FPL rules (2 GKP, 5 DEF, 5 MID, 3 FWD). 2) No single team has more than the allowed limit (usually 3) of players in the solution. 3) All selected players are unique.",
            "status": "pending",
            "testStrategy": "Run the full property-based suite to ensure thousands of random scenarios adhere to strict formation rules.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "20",
        "title": "Enhance Health Check Endpoints",
        "description": "Expand the health API to check DB connectivity, ML model status, and memory usage.",
        "details": "Update `backend/app/api/health.py`. Add checks for: 1. Database `SELECT 1`. 2. ML Engine status (verify `validate_models_loaded` passed). 3. System memory usage (optional, using `psutil`). Return a composite JSON status `{ 'status': 'ok', 'components': { 'db': 'up', 'ml': 'up' } }`. Ensure 503 status code if critical components are down.",
        "testStrategy": "Integration test: Simulate a DB downtime (e.g., close connection) and verify `/health` returns 503 and the specific component error.",
        "priority": "medium",
        "dependencies": [
          "14",
          "16"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add dependencies and prepare health module structure",
            "description": "Ensure psutil is available and structure the health endpoint file to support modular checks.",
            "dependencies": [],
            "details": "Check `backend/requirements.txt` (or equivalent) for `psutil` and add it if missing. Review `backend/app/api/health.py` and prepare imports for the Database session factory and the ML Engine service to be used in subsequent steps.",
            "status": "pending",
            "testStrategy": "Verify that `import psutil` works in the backend environment without errors.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Database connectivity check logic",
            "description": "Create a function to verify database availability by executing a lightweight query.",
            "dependencies": [
              1
            ],
            "details": "Implement an async function (e.g., `check_db_health`) in `backend/app/api/health.py`. It should acquire a DB session and execute `SELECT 1`. If successful, return status 'up'; catch exceptions (e.g., `SQLAlchemyError`) to return 'down' and log the error.",
            "status": "pending",
            "testStrategy": "Unit test mocking the DB session to simulate both successful connection and connection failure (raising an exception).",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement ML Engine status check logic",
            "description": "Create a function to verify that the Machine Learning models are loaded and ready.",
            "dependencies": [
              1
            ],
            "details": "Implement a function (e.g., `check_ml_health`) that interfaces with the ML service. It should call `validate_models_loaded` or check the singleton's initialization state. Return 'up' if models are ready, 'down' otherwise.",
            "status": "pending",
            "testStrategy": "Unit test mocking the ML service to return True/False for model readiness and verifying the output status string.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement System Resource monitoring logic",
            "description": "Create a function to retrieve current system memory usage metrics using psutil.",
            "dependencies": [
              1
            ],
            "details": "Implement `check_system_health` using `psutil.virtual_memory()`. Return a dictionary containing memory usage details (e.g., percent used). This component is considered optional/informational and should not trigger a 503 unless critical thresholds are defined.",
            "status": "pending",
            "testStrategy": "Unit test verifying that the function returns a dictionary with valid 'memory_usage' keys.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Aggregate checks into Health API response",
            "description": "Combine DB, ML, and System checks into the final endpoint and handle HTTP status codes.",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "Update the GET route in `backend/app/api/health.py` to await all check functions. Construct the composite JSON `{ 'status': 'ok'|'error', 'components': { 'db': ..., 'ml': ... } }`. Logic: If 'db' or 'ml' is 'down', set the HTTP response status code to 503 Service Unavailable.",
            "status": "pending",
            "testStrategy": "Integration test: Simulate DB down (503), ML down (503), and all systems nominal (200). Verify JSON structure matches requirements.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "21",
        "title": "Standardize Exception Handling",
        "description": "Implement a centralized exception handling mechanism to prevent silent failures and ensure consistent API errors.",
        "details": "Create a custom exception hierarchy (base `AppException`). Implement an exception handler in the API framework (e.g., FastAPI `@app.exception_handler`). Ensure exceptions from ETL, Solver, and ML components map to appropriate HTTP status codes (e.g., SolverTimeout -> 408, ModelError -> 503).",
        "testStrategy": "Unit test the exception handler. Trigger various internal exceptions via an endpoint and verify the JSON response structure and HTTP status codes.",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define Base Exception Hierarchy",
            "description": "Create a unified base exception class and specific subclasses for the application domain.",
            "dependencies": [],
            "details": "Create a new module `backend/app/core/exceptions.py`. Define a base class `AppException(Exception)` that includes fields for `message`, `error_code`, and `status_code`. Derive specific exceptions like `ResourceNotFound`, `ValidationError`, `AuthenticationError`, `ETLError`, `SolverError`, and `ModelPredictionError` from this base class to cover the ETL, Solver, and ML domains.",
            "status": "pending",
            "testStrategy": "Unit tests ensuring custom exceptions can be instantiated with correct default codes and messages.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Global Exception Handlers",
            "description": "Register global exception handlers in the FastAPI application to intercept custom exceptions.",
            "dependencies": [
              1
            ],
            "details": "In `backend/app/main.py` (or a dedicated `backend/app/core/errors.py` if modular), create functions decorated with `@app.exception_handler(AppException)`. The handler should extract the status code and error details from the exception and return a standardized `JSONResponse` with a consistent schema (e.g., `{\"error\": {\"code\": \"...\", \"message\": \"...\"}}`). Also add a catch-all handler for unhandled `Exception` cases to return a 500 Internal Server Error.",
            "status": "pending",
            "testStrategy": "Unit test the handler functions directly. Mock the app context to verify the response format.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Refactor ETL Service Exception Handling",
            "description": "Update the ETL service to use the new exception hierarchy.",
            "dependencies": [
              1,
              2
            ],
            "details": "Review `backend/app/services/etl/service.py` and related files. Replace generic `raise Exception` or `HTTPException` calls with specific exceptions like `ETLError` or `ResourceNotFound`. Ensure that upstream API failures or database integrity errors during ETL processes are caught and re-raised as domain-specific exceptions.",
            "status": "pending",
            "testStrategy": "Integration tests triggering ETL failures (e.g., external API down) and asserting the API returns the correct mapped HTTP status code (e.g., 502 or 503).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Refactor Solver and ML Service Exception Handling",
            "description": "Update Solver and ML components to raise standardized exceptions.",
            "dependencies": [
              1,
              2
            ],
            "details": "Modify the Solver service and ML prediction routines. Raise `SolverTimeout` (mapping to 408) if optimization takes too long or is infeasible. Raise `ModelError` (mapping to 503) if the ML model fails to load or predict. Ensure these components do not leak implementation details (like stack traces) in the error message.",
            "status": "pending",
            "testStrategy": "Unit tests for Solver/ML services mocking failure conditions and verifying the specific custom exception is raised.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Verify API Error Responses",
            "description": "Add integration tests to validate the end-to-end error handling behavior.",
            "dependencies": [
              3,
              4
            ],
            "details": "Create a new test file `backend/tests/api/test_errors.py`. Implement test cases that hit various endpoints with invalid data, non-existent IDs, or simulated internal failures. Assert that the HTTP response status codes match expectations (400, 404, 500, etc.) and the JSON body strictly follows the standardized error schema defined in Subtask 2.",
            "status": "pending",
            "testStrategy": "Run the new test suite against the running application or TestClient to confirm no 500 errors leak raw stack traces.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "22",
        "title": "Create E2E Prediction Pipeline Test",
        "description": "Implement an end-to-end test covering the flow from FPL API data ingestion to Frontend response.",
        "details": "Create `tests/e2e/test_pipeline.py`. Simulate the full flow: 1. Trigger ETL (mock external FPL API). 2. Verify DB state. 3. Call Prediction/Optimization endpoint. 4. Verify JSON response structure and validity. This validates the integration of all components.",
        "testStrategy": "Run the E2E test in the CI/CD pipeline. It serves as the final gate before deployment.",
        "priority": "medium",
        "dependencies": [
          "14",
          "16",
          "18",
          "20",
          "21"
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Setup E2E Test Scaffolding and Mock FPL API Data",
            "description": "Initialize the `tests/e2e/test_pipeline.py` file and create fixtures for mocking the external FPL API responses required for the ETL process.",
            "dependencies": [],
            "details": "Create the test file structure using pytest. Define JSON fixtures matching the FPL API schema (bootstrap-static, element-summary) to simulate gameweek data. Configure `requests-mock` or `respx` to intercept calls to `https://fantasy.premierleague.com/api/` and return these fixtures. Ensure the mock environment is isolated.",
            "status": "pending",
            "testStrategy": "Run `pytest tests/e2e/test_pipeline.py` and verify that the mocked endpoints are reachable and return the expected data without hitting the real API.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement ETL Trigger and Database Verification Logic",
            "description": "Write the test logic to trigger the ETL pipeline using the mocked data and verify that the data is correctly persisted in the test database.",
            "dependencies": [
              1
            ],
            "details": "Import the main ETL execution function (e.g., from `backend.app.services.etl_service`). Call this function within the test. After execution, query the test database (PostgreSQL) using SQLAlchemy to verify that players, teams, and fixtures have been inserted/updated correctly based on the mock data.",
            "status": "pending",
            "testStrategy": "Assert that row counts in the database match the mock data size and check specific field values (e.g., player names, costs) for accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Prediction Model Execution in Test Flow",
            "description": "Extend the E2E test to trigger the prediction generation process based on the ingested data.",
            "dependencies": [
              2
            ],
            "details": "Invoke the prediction service (e.g., `backend.app.services.prediction_service.generate_predictions`) immediately after the ETL verification. This step should populate the `predictions` table in the database. Verify that predictions exist for the upcoming gameweek defined in the mock data.",
            "status": "pending",
            "testStrategy": "Assert that the predictions table is not empty and that prediction scores are within valid ranges (e.g., non-negative floats).",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test Optimization Endpoint Response",
            "description": "Simulate an API client request to the optimization/team-selection endpoint and validate the response against the generated predictions.",
            "dependencies": [
              3
            ],
            "details": "Use `TestClient` from `fastapi.testclient` to send a POST/GET request to the team optimization endpoint (e.g., `/api/v1/optimization/optimize`). Ensure the request parameters (budget, positions) align with the mock data. Capture the JSON response.",
            "status": "pending",
            "testStrategy": "Check for HTTP 200 status code. Validate that the response body contains a valid team structure (11 starting players, 4 subs) and that the total cost is within the budget.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Finalize E2E Pipeline Assertions and Cleanup",
            "description": "Add comprehensive assertions for the full data lifecycle and ensure test database cleanup after execution.",
            "dependencies": [
              4
            ],
            "details": "Verify the consistency of the entire flow: Input Mock Data -> DB State -> Prediction Output -> API Response. Ensure the team returned by the API actually consists of players from the mock data. Implement a teardown fixture to clear the test database tables to prevent side effects on other tests.",
            "status": "pending",
            "testStrategy": "Run the full E2E suite. Confirm that the test passes reliably and leaves the database clean. Verify execution time is reasonable for CI inclusion.",
            "parentId": "undefined"
          }
        ]
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-01-26T00:27:52.882Z",
      "taskCount": 10,
      "completedCount": 3,
      "tags": [
        "master"
      ]
    }
  }
}